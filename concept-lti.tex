\section{Introduction to LTI Systems}
\subsection{Examples}
\begin{itemize}
	\item Heat equation observation
\end{itemize}
\subsection{State Space Systems}
\begin{align}\label{eq:lti}
	Ex(t) &= Ax(t) + Bu(t), \quad x(0) = x_0, \\
	y(t) &= Cx(t),
\end{align}
where 
\begin{itemize}
	\item $x(t) \in \Rn$: the system's state
	\item $u(t) \in \Rm$: the input or control
	\item $y(t) \in \Rq$: the output or measurements
	\item $E \in \Rnn$ is often the identity or the mass matrix of a discretization
	\item $A \in \Rnn$: the system matrix
	\item $B \in \Rnm$: the input matrix
	\item $C \in \Rqn$: the output matrix
\end{itemize}

We denote the LTI by $\abcsys$

Sometimes the state $x$ has a meaning in the model - like the temperature. Sometimes $x$ is not available or simply not of interest. Sometimes $x$ is just a mathematical object as a part of the model. 

\subsection{Transferfunctions}
The state is not accessible, not of interest or only an artificial object of the model for the input to output behavior $\mathbf G$ of a system $\mathbf P$
\begin{figure}[h]
\centering
\begin{tikzpicture}
 \draw [->] (3,1.1/2) -- (4,1.1/2); 
 \draw (4,0) rectangle (6,1.1) node[pos=.5] {$\mathbf P$};
 % \draw (4,0) rectangle (6,1.1) node[pos=.5] {\small{$y(t) = Cx(t), Ex(t) = Ax(t) + Bu(t)$}};
 \draw [->] (6,1.1/2) -- (7,1.1/2); 
 \node at (2.7,0.5) {$u$};
 \node at (7.3,0.5) {$y$};
\end{tikzpicture}
\end{figure}
that maps an input $u$ to the corresponding output $y$.

If $\mathbf P$ is an $\abcsys$, the function $\mathbf G$ can be defined via 
\begin{equation*}
	\mathbf P \colon u \mapsto y\colon y(t) = S(t, u)
\end{equation*}
this is time-domain. A function $u \colon [0, \infty) \to \Rm$ is mapped to a function $y\colon [0, \infty)  \to \Rq$.

Through the Laplace transform $\mathcal L$ and its inverse $\mathcal L^{-1}$, we can switch between time-domain and frequency-domain representations of the input and output signals.

\begin{equation*}
	U(s) := \mathcal L\{u\}(s) := \int_0^\infty e^{-st}u(t)\inva t,
\end{equation*}
where $s\in \mathbb C$ is the \emph{frequency} and
\begin{equation*}
	y(t) := \mathcal L^{-1}\{Y\}(t) := \lim_{T\to \infty} \frac 1{2\pi i} \int_{\gamma - iT}^{\gamma + iT} e^{s}Y(s)\inva s
\end{equation*}
where $\gamma \in \R$ is chosen such that contour path of the integration is the domain of convergence of $Y$.

With the basic properties of the Laplace transform
\begin{itemize}
	\item $\dot X(s):= \mathcal L\{\dot x\}(s) -x(0)= s\mathcal L\{x\}(s) = s X(s)-x(0)$
	\item and linearity $\mathcal L\{Ax\}(s) = AX(s)$
\end{itemize}
with zero initial value $x(0) = 0$, the $\abcsys$ defines the transferfunction
\begin{equation*}
	G(s) := C(sE-A)^{-1}B
\end{equation*}

\subsection{Realizations}
Given a rational matrix function $s\mapsto G(s)\in \mathbb R^{q\times m}$, is there $\abcsys$ so that 
\begin{equation*}
	G(s) = C(sI-A)^{-1}B ?
\end{equation*}
If there is one such state space system $\abcsys$, then there are infinitely many:

For every invertible $T$, the state space system $\abcsyst$ is a realization, since
\begin{equation*}
	C(sI-A)^{-1}B = CT^{-1}(s-TAT^{-1})^{-1}TB.
\end{equation*}

Also
\begin{equation*}
	(
	\begin{bmatrix} A & 0 \\ 0 & 0 \end{bmatrix},
	\begin{bmatrix} B \\ 0 \end{bmatrix},
	\begin{bmatrix} C & 0 \end{bmatrix}, -
	)
\end{equation*}
is a realization of one and the same transferfunction.

Thus, there are many possible state space representations of a transfer function. Which one is a good choice? The dimension can be arbitrary large. How small can it be? (cf. \emph{model reduction})

\subsection{Controllability, Observability}

The states of an LTI are just part of a model that realizes a transfer function $G$. A transferfunction $G$ describes how controls $u$ lead to outputs $y$. As seen above in the example, there can be states that are neither affected (\emph{controlled} by the inputs nor seen (\emph{observed}) by the outputs of the system and which are obviously not needed to realize the input to output behavior of $G$.  

We will give a thorough characterization of the \emph{controllable} and \emph{observable} states of an LTI. 

\begin{definition}
	The LTI $\abcsys$ or the pair $(A, B)$ is said to be \emph{controllable} if, for any initial state $x(0) = x_0$, $t_1 > 0$ and final state $x_1$, there exists a (piecewise continuous) input $u$ such that the solution of \eqref{eq:lti} satisfies $x(t_1 ) = x_1$. Otherwise, the system $\abcsys$ or the pair $(A, B)$ is said to be \emph{uncontrollable}.
\end{definition}


\begin{definition}
	The LTI \abcsys~ or the pair $(C,A)$ is said to be \emph{observable} if, for any $t_1 > 0$, the initial state $x(0) = x_0$ can be determined from the time history of the input $u$ and the output $y$ in the interval of $[0, t_1]$. Otherwise, the system \abcsys, or $(C, A)$, is said to be \emph{unobservable}.
\end{definition}

There are algebraic conditions on controllability and observability.

\begin{theorem}
	The LTI $\abcsys$ is controllable (observable) if, and only if, the transformed LTI $\abcsyst$ is controllable (observable), where $T$ is a regular matrix.
\end{theorem}

With these definitions, and the invariance of the properties and the transferfunction with respect to state space transformations, we can find the states that really matter for the realization of a transferfunction.

\begin{theorem}[Kalman Canonical Decomposition]
	Given an LTI $\abcsys$, there is a state space transformation $T$ such that the transformed system $\abcsyst$ has the form 

	\begin{align*}
		\frac {\inva{}}{\inva t}
		\begin{bmatrix} x_{co} \\ x_{c\bar o} \\ x_{\bar c o} \\ x_{\bar c \bar o} \end{bmatrix} 
			&= 
		\begin{bmatrix}
			A\sco & 0 & A_{13} & 0 \\
			A_{21} & A\scbo & A_{23} & A_{24} \\
			0 & 0 & A\sbco & 0 \\
			0 & 0 & A_{43} & A\sbcbo 
		\end{bmatrix}
		\begin{bmatrix} x_{co} \\ x_{c\bar o} \\ x_{\bar c o} \\ x_{\bar c \bar o} \end{bmatrix}
			+
		\begin{bmatrix} B\sco \\ B\scbo \\ 0 \\ 0 \end{bmatrix} u \\
			y &= 
			\begin{bmatrix} C\sco & 0 & C\sbco & 0 \end{bmatrix}
		\begin{bmatrix} x_{co} \\ x_{c\bar o} \\ x_{\bar c o} \\ x_{\bar c \bar o} \end{bmatrix},
	\end{align*}
	with the subsystem $(A\sco, B\sco, C\sco, -)$ being controllable and observable, while the remaining states $x\sbco$, $x\scbo$, or $x\sbcbo$ are not controllable, not observable, or neither of them.
	Moreover, for the transferfunction, it holds that
	\begin{equation*}
		G(s) = C(sI-A)^{-1}B = C\sco(sI-A\sco)^{-1}B\sco.
	\end{equation*}
\end{theorem}

What does this mean for us and a transferfunction $G(s)$?
\begin{itemize}
	\item The minimal dimension of a realization is the dimension of $x\sco$ in the \emph{Kalman Canonical Decomposition}
	\item Such a realization is called \emph{minimal realization}
	\item It is the starting point for further model reduction. (Throwing out $x\sbco$ etc. does not effect $G(s)$ and is typically not considered a model reduction)
	\item There are algorithm to reduce a realization to a minimal one (cf. Varga)
	\item In practice, the uncontrolled and unobserved states play a role and they may cause troubles ( check the literature for \emph{zero dynamics} )
\end{itemize}

\subsection{Stability}
A system $G$ is stable if all \emph{poles} of $G$ are located in the left half-plane $\C^-$.

{\footnotesize If $m=q=1$, then $G(s)=\frac{N(s)}{D(s)}$, where $N(s)$ and $D(s)$ are polynomials and the \emph{poles} are the roots of $D(s)$, i.e. those $s\in\mathbb C$ for which $D(s)=0$.

If $m$, $q >1$, then one can use the \emph{McMillan} form of $G$ to determine the poles.}

If \abcdsys~is a minimal realization of a stable system $G$, then the poles of $G$ are the eigenvalues of $A$.

In this case, the system is stable if for all eigenvalues $\lambda$ of $A$, it holds that $\lambda \in \C^-$. Then $A$ is called \emph{stable} or \emph{Hurwitz}.

A stable system can have a stable realization.


\subsection{Gramians and Balanced Realizations}
If $A$ is stable, then the \emph{Lyapunov} equations 
\begin{align*}
	A^*P + PA + BB^* = 0
	\intertext{and}
	AQ + Q^*A + C^*C = 0
\end{align*}
have a unique positive definite solutions $P$ and $Q$. The matrix $P$ is called the the controllability Gramian and $Q$ is called the observability Gramian.

If $P$ and $Q$ are the Gramians of a stable realization \abcdsys, then the transformed system $\rabcdsys=\abcdsyst$ has the Gramians 
\begin{equation*}
	\hat P = TPT^* \quad \text{and} \quad \hQ = (T^{-1})^*QT^{-1}
\end{equation*}
for any regular transformation $T$.

For any minimal and stable system $\abcdsys$, there are particular transformations $T$, so that the transformed system has Gramians that are equal and diagonal:
\begin{equation*}
	\hat P = \hQ =
	\begin{bmatrix}
		\lambda_1 & & & \\
		& \lambda_2 & & \\
		& & \ddots & \\
		& & & \lambda_n \\
	\end{bmatrix},
\end{equation*}
with $\lambda_1 \geq \lambda_2 \geq \dotsm \geq \lambda_n > 0$.

These realizations are called \emph{Balanced Realizations}.

